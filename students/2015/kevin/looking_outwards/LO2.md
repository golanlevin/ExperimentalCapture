# Looking Outwards 2: Temporal Synchronization of Multiple Audio Signals

This Looking Outward reflects on the paper [Temporal Synchronization of Multiple Audio Signals](LO2.pdf).

The research team investigated two ways to synchronize the audio across multiple temporally distinct recordings.  Given the proliferation of devices that can record audio resulting in multiple streams of the same event, the ability to stitch these recordings together can provide new contextual information for all recordings involved, and allow users to experience their recording in a way potentially even richer than their initial experience.

The paper appears to have only examined the efficiency of stitching together audio files which were actually recorded from the same source, and didn't analyze the algorithms ability to filter audio files which were of a different subject.  Most interesting applications of this research I can think of \(such as stitching together audio files recorded within a single hashtag on twitter for a concert\) would require the ability to filter out files which have the same identifier but in fact be a false recording.

One of the interesting aspects of the bibliography is that many of the papers cited relate to multiple camera feeds, as opposed to the audio in isolation.  I wonder what additional research has been done in the audio exclusive environment, and whether given the fact that most consumer devices such as smart phones record audio and video, whether there's a great benefit to the vacuum approach of stitching together audio only.