#Looking Outwards 1

## Google Deep Dream

![puppiesAndPasta](https://kver.files.wordpress.com/2015/07/spaghetti-meatballs-become-really-frightening.jpg)

The output of Google’s DeepDream neural network has become very recognizable this past summer. The network was originally used to classify images, since the code has been released has now been re-purposed to make images out of nothing or entirely different. 

![pic](http://2.bp.blogspot.com/-17ajatawCW4/VYITTA1NkDI/AAAAAAAAAlM/eZmy5_Uu9TQ/s1600/classvis.png)


The network identifies features in images that could possibly resemble what it is looking for. It then emphasizes the elements it recognizes, over and over again, so its like inverting it from what it originally was supposed to do.

An artificial neural network is based off of our brains but it really doesn’t come close to the brain's complexity. But at a base level of both there are layers of ’neurons’ and ‘connections’. You need at lease 2MB of data to train the whole network on. (Google deep dream was trained on 1,200,000MB (1.3 TB) of data) and what the data does is that is adds “weight” between different connections. So basically its associating inputs with outputs. Thats like taking a kid and pointing at a picture of a cat and telling it “CAT” and showing it a dog and saying “DOG” but for a massive amount of data.

So with all of this results in a completely new output that is not like any of the inputs but can (hopefully) pass as an element of the input.

If your interested here are more links to more info:

[Google's explination] (http://googleresearch.blogspot.ch/2015/06/inceptionism-going-deeper-into-neural.html)

[Memo Akten gives a really cool explination] (https://medium.com/@memoakten/deepdream-is-blowing-my-mind-6a2c8669c698)

### Other notes

Despite all this talk on its use on images there are many more interesting uses on text based data. There are many ways that images are represented in text form, for example vector line drawings. There are really cool examples of getting an ANN to make [recipes, write Shakespeare,](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) or play chess.
 
