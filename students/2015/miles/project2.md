[![Stomach Cut](images/stomach_cut.png)](https://vimeo.com/142858398 "Stomach Cut")
*Video [here](https://vimeo.com/142858398)*


A robotic surgery system encounters a person whose field of vision is covered with floaters. The video explores how entoptic phenomena – visual effects whose source is the eye itself – create private worlds. 

I used my hand to control the movements of the da Vinci Robot, to try to mimic how the robot is used by surgeons. The human model in the video was made from images and depth data of my body, which I captured in a dome of ten Kinects. The other model in the video is a sensory neuron that was generated by gamers playing Eyewire, a human-based computation game. 
  
After I got my scan, I was very excited. I had been waiting for a long time to play with a scan of myself. I started to wonder what people might do with scans of themselves as 3d scanning technology becomes more available. I became fixated on the idea of seppuku simulator: What would it feel like? It’s common to die in video games, but it’s less common to watch an accurate representation of yourself die. That being said, I’m not trying to depict a death or suicide in this video.

Another idea that comes up in this piece is the connection between technology and the senses. The da Vinci robot gives the surgeon eyes and hands that can travel in the patients’ body, and the visualization of the neuron from microscopy data functions similarly. It is worth noting that the Leonardo da Vinci produced many anatomical drawings. His studies of anatomy led to the creation of one of the earliest robots, made around the year 1495. Just as artificial intelligence researchers look to neuroscience in order to model the mind, there is an exchange of knowledge between anatomy and robotics. 

I enjoyed working with 3D models that were scans of real objects, as opposed to models that were generated procedurally or according to a blueprint. I want to be attentive to the specific material origin of data as I develop my work.
